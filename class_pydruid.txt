class PyDruid(BaseDruidClient)
 |  PyDruid contains the functions for creating and executing Druid queries. Returns Query objects that can be used
 |  for exporting query results into TSV files or pandas.DataFrame objects for subsequent analysis.
 |  
 |  :param str url: URL of Broker node in the Druid cluster
 |  :param str endpoint: Endpoint that Broker listens for queries on
 |  
 |  Example
 |  
 |  .. code-block:: python
 |      :linenos:
 |  
 |          >>> from pydruid.client import *
 |  
 |          >>> query = PyDruid('http://localhost:8083', 'druid/v2/')
 |  
 |          >>> top = query.topn(
 |                  datasource='twitterstream',
 |                  granularity='all',
 |                  intervals='2013-10-04/pt1h',
 |                  aggregations={"count": doublesum("count")},
 |                  dimension='user_name',
 |                  filter = Dimension('user_lang') == 'en',
 |                  metric='count',
 |                  threshold=2
 |              )
 |  
 |          >>> print json.dumps(top.query_dict, indent=2)
 |          >>> {
 |                "metric": "count",
 |                "aggregations": [
 |                  {
 |                    "type": "doubleSum",
 |                    "fieldName": "count",
 |                    "name": "count"
 |                  }
 |                ],
 |                "dimension": "user_name",
 |                "filter": {
 |                  "type": "selector",
 |                  "dimension": "user_lang",
 |                  "value": "en"
 |                },
 |                "intervals": "2013-10-04/pt1h",
 |                "dataSource": "twitterstream",
 |                "granularity": "all",
 |                "threshold": 2,
 |                "queryType": "topN"
 |              }
 |  
 |          >>> print top.result
 |          >>> [{'timestamp': '2013-10-04T00:00:00.000Z',
 |              'result': [{'count': 7.0, 'user_name': 'user_1'}, {'count': 6.0, 'user_name': 'user_2'}]}]
 |  
 |          >>> df = top.export_pandas()
 |          >>> print df
 |          >>>    count                 timestamp      user_name
 |              0      7  2013-10-04T00:00:00.000Z         user_1
 |              1      6  2013-10-04T00:00:00.000Z         user_2
 |  
 |  Method resolution order:
 |      PyDruid
 |      BaseDruidClient
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, url, endpoint)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from BaseDruidClient:
 |  
 |  export_pandas(self)
 |      Export the current query result to a Pandas DataFrame object.
 |      
 |      .. deprecated::
 |          Use Query.export_pandas() method instead
 |  
 |  export_tsv(self, dest_path)
 |      Export the current query result to a tsv file.
 |      
 |      .. deprecated::
 |          Use Query.export_tsv() method instead.
 |  
 |  groupby(self, **kwargs)
 |      A group-by query groups a results set (the requested aggregate metrics) by the specified dimension(s).
 |      
 |      Required key/value pairs:
 |      
 |      :param str datasource: Data source to query
 |      :param str granularity: Time bucket to aggregate data by hour, day, minute, etc.,
 |      :param intervals: ISO-8601 intervals for which to run the query on
 |      :type intervals: str or list
 |      :param dict aggregations: A map from aggregator name to one of the pydruid.utils.aggregators e.g., doublesum
 |      :param list dimensions: The dimensions to group by
 |      
 |      :return: The query result
 |      :rtype: Query
 |      
 |      Optional key/value pairs:
 |      
 |      :param pydruid.utils.filters.Filter filter: Indicates which rows of data to include in the query
 |      :param pydruid.utils.having.Having having: Indicates which groups in results set of query to keep
 |      :param post_aggregations:   A dict with string key = 'post_aggregator_name', and value pydruid.utils.PostAggregator
 |      :param dict context: A dict of query context options
 |      :param dict limit_spec: A dict of parameters defining how to limit the rows returned, as specified in the Druid api documentation
 |      
 |      Example:
 |      
 |      .. code-block:: python
 |          :linenos:
 |      
 |              >>> group = client.groupby(
 |                      datasource='twitterstream',
 |                      granularity='hour',
 |                      intervals='2013-10-04/pt1h',
 |                      dimensions=["user_name", "reply_to_name"],
 |                      filter=~(Dimension("reply_to_name") == "Not A Reply"),
 |                      aggregations={"count": doublesum("count")},
 |                      context={"timeout": 1000}
 |                      limit_spec={
 |                          "type": "default",
 |                          "limit": 50,
 |                          "columns" : ["count"]
 |                      }
 |                  )
 |              >>> for k in range(2):
 |                  ...     print group[k]
 |              >>> {'timestamp': '2013-10-04T00:00:00.000Z', 'version': 'v1', 'event': {'count': 1.0, 'user_name': 'user_1', 'reply_to_name': 'user_2'}}
 |              >>> {'timestamp': '2013-10-04T00:00:00.000Z', 'version': 'v1', 'event': {'count': 1.0, 'user_name': 'user_2', 'reply_to_name': 'user_3'}}
 |  
 |  segment_metadata(self, **kwargs)
 |      A segment meta-data query returns per segment information about:
 |      
 |      * Cardinality of all the columns present
 |      * Column type
 |      * Estimated size in bytes
 |      * Estimated size in bytes of each column
 |      * Interval the segment covers
 |      * Segment ID
 |      
 |      Required key/value pairs:
 |      
 |      :param str datasource: Data source to query
 |      :param intervals: ISO-8601 intervals for which to run the query on
 |      :type intervals: str or list
 |      
 |      Optional key/value pairs:
 |      
 |      :param dict context: A dict of query context options
 |      
 |      :return: The query result
 |      :rtype: Query
 |      
 |      Example:
 |      
 |      .. code-block:: python
 |          :linenos:
 |      
 |              >>> meta = client.segment_metadata(datasource='twitterstream', intervals = '2013-10-04/pt1h')
 |              >>> print meta[0].keys()
 |              >>> ['intervals', 'id', 'columns', 'size']
 |              >>> print meta[0]['columns']['tweet_length']
 |              >>> {'errorMessage': None, 'cardinality': None, 'type': 'FLOAT', 'size': 30908008}
 |  
 |  select(self, **kwargs)
 |      A select query returns raw Druid rows and supports pagination.
 |      
 |      Required key/value pairs:
 |      
 |      :param str datasource: Data source to query
 |      :param str granularity: Time bucket to aggregate data by hour, day, minute, etc.
 |      :param dict paging_spec: Indicates offsets into different scanned segments
 |      :param intervals: ISO-8601 intervals for which to run the query on
 |      :type intervals: str or list
 |      
 |      Optional key/value pairs:
 |      
 |      :param pydruid.utils.filters.Filter filter: Indicates which rows of data to include in the query
 |      :param list dimensions: The list of dimensions to select. If left empty, all dimensions are returned
 |      :param list metrics: The list of metrics to select. If left empty, all metrics are returned
 |      :param dict context: A dict of query context options
 |      
 |      :return: The query result
 |      :rtype: Query
 |      
 |      Example:
 |      
 |      .. code-block:: python
 |          :linenos:
 |      
 |              >>> raw_data = client.select(
 |                      datasource=twitterstream,
 |                      granularity='all',
 |                      intervals='2013-06-14/pt1h',
 |                      paging_spec={'pagingIdentifies': {}, 'threshold': 1},
 |                      context={"timeout": 1000}
 |                  )
 |              >>> print raw_data
 |              >>> [{'timestamp': '2013-06-14T00:00:00.000Z', 'result': {'pagingIdentifiers': {'twitterstream_2013-06-14T00:00:00.000Z_2013-06-15T00:00:00.000Z_2013-06-15T08:00:00.000Z_v1': 1, 'events': [{'segmentId': 'twitterstream_2013-06-14T00:00:00.000Z_2013-06-15T00:00:00.000Z_2013-06-15T08:00:00.000Z_v1', 'offset': 0, 'event': {'timestamp': '2013-06-14T00:00:00.000Z', 'dim': 'value'}}]}}]
 |  
 |  time_boundary(self, **kwargs)
 |      A time boundary query returns the min and max timestamps present in a data source.
 |      
 |      Required key/value pairs:
 |      
 |      :param str datasource: Data source to query
 |      
 |      Optional key/value pairs:
 |      
 |      :param dict context: A dict of query context options
 |      
 |      :return: The query result
 |      :rtype: Query
 |      
 |      Example:
 |      
 |      .. code-block:: python
 |          :linenos:
 |      
 |              >>> bound = client.time_boundary(datasource='twitterstream')
 |              >>> print bound
 |              >>> [{'timestamp': '2011-09-14T15:00:00.000Z', 'result': {'minTime': '2011-09-14T15:00:00.000Z', 'maxTime': '2014-03-04T23:44:00.000Z'}}]
 |  
 |  timeseries(self, **kwargs)
 |      A timeseries query returns the values of the requested metrics (in aggregate) for each timestamp.
 |      
 |      Required key/value pairs:
 |      
 |      :param str datasource: Data source to query
 |      :param str granularity: Time bucket to aggregate data by hour, day, minute, etc.,
 |      :param intervals: ISO-8601 intervals for which to run the query on
 |      :type intervals: str or list
 |      :param dict aggregations: A map from aggregator name to one of the pydruid.utils.aggregators e.g., doublesum
 |      
 |      :return: The query result
 |      :rtype: Query
 |      
 |      Optional key/value pairs:
 |      
 |      :param pydruid.utils.filters.Filter filter: Indicates which rows of data to include in the query
 |      :param post_aggregations:   A dict with string key = 'post_aggregator_name', and value pydruid.utils.PostAggregator
 |      :param dict context: A dict of query context options
 |      
 |      Example:
 |      
 |      .. code-block:: python
 |          :linenos:
 |      
 |              >>> counts = client.timeseries(
 |                      datasource=twitterstream,
 |                      granularity='hour',
 |                      intervals='2013-06-14/pt1h',
 |                      aggregations={"count": doublesum("count"), "rows": count("rows")},
 |                      post_aggregations={'percent': (Field('count') / Field('rows')) * Const(100))},
 |                      context={"timeout": 1000}
 |                  )
 |              >>> print counts
 |              >>> [{'timestamp': '2013-06-14T00:00:00.000Z', 'result': {'count': 9619.0, 'rows': 8007, 'percent': 120.13238416385663}}]
 |  
 |  topn(self, **kwargs)
 |      A TopN query returns a set of the values in a given dimension, sorted by a specified metric. Conceptually, a
 |      topN can be thought of as an approximate GroupByQuery over a single dimension with an Ordering spec. TopNs are
 |      faster and more resource efficient than GroupBy for this use case.
 |      
 |      Required key/value pairs:
 |      
 |      :param str datasource: Data source to query
 |      :param str granularity: Aggregate data by hour, day, minute, etc.,
 |      :param intervals: ISO-8601 intervals of data to query
 |      :type intervals: str or list
 |      :param dict aggregations: A map from aggregator name to one of the pydruid.utils.aggregators e.g., doublesum
 |      :param str dimension: Dimension to run the query against
 |      :param str metric: Metric over which to sort the specified dimension by
 |      :param int threshold: How many of the top items to return
 |      
 |      :return: The query result
 |      :rtype: Query
 |      
 |      Optional key/value pairs:
 |      
 |      :param pydruid.utils.filters.Filter filter: Indicates which rows of data to include in the query
 |      :param post_aggregations:   A dict with string key = 'post_aggregator_name', and value pydruid.utils.PostAggregator
 |      :param dict context: A dict of query context options
 |      
 |      Example:
 |      
 |      .. code-block:: python
 |          :linenos:
 |      
 |              >>> top = client.topn(
 |                          datasource='twitterstream',
 |                          granularity='all',
 |                          intervals='2013-06-14/pt1h',
 |                          aggregations={"count": doublesum("count")},
 |                          dimension='user_name',
 |                          metric='count',
 |                          filter=Dimension('user_lang') == 'en',
 |                          threshold=1,
 |                          context={"timeout": 1000}
 |                      )
 |              >>> print top
 |              >>> [{'timestamp': '2013-06-14T00:00:00.000Z', 'result': [{'count': 22.0, 'user': "cool_user"}}]}]
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from BaseDruidClient:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)

None
